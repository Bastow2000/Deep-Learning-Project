{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PreProcessing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "\n",
    "    # Waveform model: sample level CNN\n",
    "    'CNN': {\n",
    "        'signal': 'spectrogram',\n",
    "        \"SampleRate\": 44100,\n",
    "        \"NumberOfBins\": 84,\n",
    "        \"BinsPerOctave\": 12, # higher the value better resolution more computationally expensive\n",
    "        \"HopLength\": 512,\n",
    "        \"ChunkSize\": {\"large\":8613, \"mid\": 4306 ,\"small\": 2153,\"verySmall\": 431},\n",
    "        \"SpectrogramDuration\": 10,\n",
    "        \"Mono\":1,\n",
    "        \"InstrumentLookup\": {1:0,  41:1, 42:2, 43:3, 72:4, 71:5, 61:6, 69:7, 74:8, 7:9, 44:10},\n",
    "        \"Instruments\" : {\"Grand Piano\":1, \"Viola\": 41, \"Cello\":42, \"Contrabass\":43, \"Piccolo\":72, \"Brass Section\":61, \"English Horn\":69, \"Recorder\":74,\"Clavinet\":7, \"Tremolo Strings\":44},\n",
    "        \"FilterShapes\": [(32, (3, 3)), (64, (5, 5)), (128, (16, 2)), (128, (2,16)), (256,(7,7))],\n",
    "        \"NumberInstruments\": 11,\n",
    "        \"NumberPitches\": 128,\n",
    "        \"BatchSize\": 16,\n",
    "        \"LearningRate\": 0.000001,\n",
    "        \"NumberEpochs\": 100,\n",
    "        \"Regularisation\": 0.00001\n",
    "        },\n",
    "    \n",
    "    'Path': {\n",
    "        \"TestAudio\": \"test_data\",\n",
    "        \"TrainAudio\": \"train_data\",\n",
    "        \"ValidAudio\": \"valid_data\",\n",
    "        \"TestLabels\": \"test_labels\",\n",
    "        \"TrainLabels\": \"train_labels\",\n",
    "        \"ValidLabels\": \"valid_labels\",\n",
    "        \"SavedModels\": \"saved_models/model.pth\"\n",
    "    }\n",
    "    # Waveform model: sample level CNN\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Main Pre processing\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    " \n",
    "class AudioProcessing:\n",
    "    def __init__(self, sampleRate, chunkSize, config):\n",
    "        self.sampleRate = sampleRate\n",
    "        self.chunkSize = chunkSize\n",
    "        self.duration = config['CNN'][\"SpectrogramDuration\"]\n",
    "        self.mono = config['CNN'][\"Mono\"]\n",
    "        self.numberOfBins = config[\"CNN\"][\"NumberOfBins\"]\n",
    "        self.binsPerOctave = config[\"CNN\"][\"BinsPerOctave\"]\n",
    "        self.hopLength = config[\"CNN\"][\"HopLength\"]\n",
    "\n",
    "    # Extracts the CQT Spectrogram from the audio files \n",
    "    def extractAudio(self, audioSignal):\n",
    "        \n",
    "        cqt = librosa.cqt(audioSignal, sr=self.sampleRate, n_bins=self.numberOfBins, bins_per_octave=self.binsPerOctave, hop_length=self.hopLength)\n",
    "        #print(f\"First 5 audio: {cqt[:5]}\")\n",
    "        return librosa.amplitude_to_db(np.abs(cqt), ref=np.max)\n",
    "    \n",
    "    # Splits the audio files into chunks\n",
    "    def chunkAudio(self, audioCQT):\n",
    "        chunkLength = (audioCQT.shape[-1] + self.chunkSize - 1) // self.chunkSize\n",
    "        paddedLength = chunkLength * self.chunkSize - audioCQT.shape[-1]\n",
    "        paddedData = np.pad(audioCQT, ((0, 0), (0, paddedLength)), mode='constant', constant_values=0)\n",
    "        return paddedData.reshape(-1, self.numberOfBins, self.chunkSize).transpose(0, 2, 1)  # Rearrange dimensions to match the expected output\n",
    "    \n",
    "    # Normalises audio files using gaussian normalisation\n",
    "    def normaliseAudio(self, audioCQT):\n",
    "        mean = np.mean(audioCQT, keepdims=True)\n",
    "        std = np.std(audioCQT, keepdims=True)\n",
    "        normalisedData = (audioCQT - mean) / std\n",
    "        return normalisedData\n",
    "            \n",
    "    # Goes through each file in the directoryPath and applies all of the functions above\n",
    "    def processAudio(self, directoryPath):\n",
    "        signals = []\n",
    "        for filename in os.listdir(directoryPath):\n",
    "            filePath = os.path.join(directoryPath, filename)\n",
    "            if filePath.endswith('.wav'):\n",
    "                signal, _ = librosa.load(filePath, sr=self.sampleRate, mono=self.mono)\n",
    "                cqtFeatures = self.extractAudio(signal)\n",
    "                chunkedFeatures = self.chunkAudio(cqtFeatures)\n",
    "                normalisedFeatures = self.normaliseAudio(chunkedFeatures)\n",
    "                signals.append(normalisedFeatures)\n",
    "                # Explicit memory management\n",
    "        return signals\n",
    "\n",
    "class LabelProcessing:\n",
    "    def __init__(self, instrumentLookup, numberPitches, chunkSize, numFrames, frameRate):\n",
    "        self.instrumentLookup = instrumentLookup\n",
    "        self.numberPitches = numberPitches\n",
    "        self.chunkSize = chunkSize\n",
    "        self.numFrames = numFrames\n",
    "        self.frameRate = frameRate\n",
    "\n",
    "    def labelsToFrames(self, labels, num_frames, frame_rate):\n",
    "        numberInstruments = len(self.instrumentLookup)\n",
    "        annotationMatrix = torch.zeros((num_frames, numberInstruments + self.numberPitches))\n",
    "\n",
    "        for label in labels:\n",
    "            startFrame = math.floor(label['start_time'] * frame_rate / self.chunkSize)\n",
    "            endFrame = math.ceil(label['end_time'] * frame_rate / self.chunkSize)\n",
    "           # print(f\"Label: {label}, Start Frame: {startFrame}, End Frame: {endFrame}\")  # Debugging print\n",
    "            \n",
    "            if label['instrument'] in self.instrumentLookup:\n",
    "                instrumentIdx = self.instrumentLookup[label['instrument']]\n",
    "                annotationMatrix[startFrame:endFrame, instrumentIdx] = 1\n",
    "            \n",
    "            if 0 <= label['note'] < self.numberPitches:\n",
    "                pitchIdx = numberInstruments + label['note']\n",
    "                annotationMatrix[startFrame:endFrame, pitchIdx] = 1\n",
    "\n",
    "        return annotationMatrix\n",
    "\n",
    "    def processLabels(self, directoryPath):\n",
    "        labels = []\n",
    "        for filename in os.listdir(directoryPath):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                labelFilePath = os.path.join(directoryPath, filename)\n",
    "                with open(labelFilePath, 'r') as csvfile:\n",
    "                    csvreader = csv.reader(csvfile)\n",
    "                    next(csvreader)  # Skip header row\n",
    "                    for row in csvreader:\n",
    "                        labels.append({\n",
    "                            'start_time': float(row[0]),\n",
    "                            'end_time': float(row[1]),\n",
    "                            'instrument': int(row[2]),\n",
    "                            'note': int(row[3]),\n",
    "                        })\n",
    "        # Debugging print to verify labels format\n",
    "        print(f\"First 5 labels: {labels[:5]}\")\n",
    "        \n",
    "        annotationMatrix = self.labelsToFrames(labels, self.numFrames, self.frameRate)\n",
    "        numInstruments = len(self.instrumentLookup)\n",
    "        instrumentMatrix = annotationMatrix[:, :numInstruments]\n",
    "        pitchMatrix = annotationMatrix[:, numInstruments:]\n",
    "        \n",
    "        return instrumentMatrix, pitchMatrix\n",
    "\n",
    "\n",
    "class PreProcessingPipeline:\n",
    "    def __init__(self, config, numFrames, frameRate):\n",
    "        self.config = config\n",
    "        self.numFrames = numFrames\n",
    "        self.frameRate = frameRate \n",
    "        self.sampleRate = config[\"CNN\"][\"SampleRate\"]\n",
    "        self.chunkSize = config[\"CNN\"][\"ChunkSize\"][\"verySmall\"]\n",
    "        self.numberPitches = config[\"CNN\"][\"NumberPitches\"]\n",
    "        self.instrumentLookup = config[\"CNN\"][\"InstrumentLookup\"]\n",
    "        self.audioProcessing = AudioProcessing(self.sampleRate, self.chunkSize, self.config)\n",
    "        self.labelProcessing = LabelProcessing(self.instrumentLookup, self.numberPitches, self.chunkSize, self.numFrames, self.frameRate)\n",
    "\n",
    "    def processAll(self, labelsDir, audioDir):\n",
    "        \n",
    "        features = self.audioProcessing.processAudio(audioDir)\n",
    "    \n",
    "        featuresArray = np.concatenate(features, axis=0)\n",
    "        featuresArray = np.expand_dims(featuresArray, axis=1)\n",
    "\n",
    "        # Convert the concatenated numpy array to a PyTorch tensor\n",
    "        featuresTensor = torch.from_numpy(featuresArray).float()\n",
    "    \n",
    "        # Process labels and get tensors for instrument and pitch matrices\n",
    "        instrumentMatrix, pitchMatrix = self.labelProcessing.processLabels(labelsDir)\n",
    "        print(instrumentMatrix.shape)\n",
    "        \n",
    "       \n",
    "        # Ensure the tensors for features, instrument labels, and pitch labels are of the same length\n",
    "        min_size = min(featuresTensor.size(0), instrumentMatrix.size(0), pitchMatrix.size(0))\n",
    "        featuresTensor = featuresTensor[:min_size]\n",
    "        instrumentMatrix = instrumentMatrix[:min_size]\n",
    "        pitchMatrix = pitchMatrix[:min_size]\n",
    "\n",
    "        print(pitchMatrix.shape)\n",
    "        print(instrumentMatrix.shape)\n",
    "       \n",
    "        print(featuresTensor.shape)\n",
    "         # Visualize the instrument activations\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.imshow(instrumentMatrix.T, aspect='auto', origin='lower', cmap='hot')\n",
    "        ax1.set_title('Instrument Activations')\n",
    "        ax1.set_xlabel('Frame Index')\n",
    "        ax1.set_ylabel('Instrument Index')\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize the pitch activations\n",
    "        fig, ax2 = plt.subplots(figsize=(10, 5))\n",
    "        ax2.imshow(pitchMatrix.T, aspect='auto', origin='lower', cmap='hot')\n",
    "        ax2.set_title('Pitch Activations')\n",
    "        ax2.set_xlabel('Frame Index')\n",
    "        ax2.set_ylabel('Pitch Index')\n",
    "        plt.show()\n",
    "    \n",
    "        # Create the dataset from tensors\n",
    "        dataset = TensorDataset(featuresTensor, instrumentMatrix, pitchMatrix)\n",
    "        print(dataset)\n",
    "        return dataset\n",
    "    \n",
    "audioLength = config['CNN'][\"SampleRate\"] * config['CNN'][\"SpectrogramDuration\"]\n",
    "numFrames = math.ceil(audioLength / config['CNN'][\"ChunkSize\"][\"verySmall\"])\n",
    "frameRate = config['CNN'][\"SampleRate\"] / config['CNN'][\"ChunkSize\"][\"verySmall\"]\n",
    "pipeline = PreProcessingPipeline(config,numFrames,frameRate)\n",
    "\n",
    "\n",
    "trainDataset = pipeline.processAll(config[\"Path\"][\"TrainLabels\"], config[\"Path\"][\"TrainAudio\"])\n",
    "\n",
    "testDataset = pipeline.processAll(config[\"Path\"][\"TestLabels\"], config[\"Path\"][\"TestAudio\"])\n",
    "\n",
    "validDataset = pipeline.processAll(config[\"Path\"][\"ValidLabels\"], config[\"Path\"][\"ValidAudio\"])\n",
    "\n",
    "\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=config[\"CNN\"][\"BatchSize\"], shuffle=True)\n",
    "testDataloader = DataLoader(testDataset, batch_size=config[\"CNN\"][\"BatchSize\"], shuffle=True,pin_memory=True)\n",
    "validDataloader = DataLoader(validDataset, batch_size=config[\"CNN\"][\"BatchSize\"], shuffle=True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskCnnModel(nn.Module):\n",
    "    \"\"\"A CNN model for multi-task learning with separate branches for pitch and instrument detection.\"\"\"\n",
    "    def __init__(self, NumberPitches, NumberInstruments, NumberOfBins):\n",
    "        super(MultiTaskCnnModel, self).__init__()\n",
    "\n",
    "        # Shared initial convolutional blocks\n",
    "        self.sharedConvBlocks = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(0.8),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(0.8)\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Pitch-specific convolutional blocks\n",
    "        self.pitchConvBlocks = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, int(0.2 * NumberOfBins)), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(0.8),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 2), padding=1),\n",
    "            nn.BatchNorm2d(256),  # Corrected to match out_channels\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(0.8)\n",
    "           \n",
    "        )\n",
    "\n",
    "        # Instrument-specific convolutional blocks\n",
    "        self.instrumentConvBlocks = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(5, int(0.25 * NumberOfBins)), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(0.8),\n",
    "            \n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(7, 1), padding=1),\n",
    "            nn.BatchNorm2d(256),  # Corrected to match out_channels\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(0.8)\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Adaptive pooling layer\n",
    "        self.adaptivePool = nn.AdaptiveAvgPool2d((1, 1)) \n",
    "\n",
    "        # Fully connected layers for pitch detection\n",
    "        self.pitch = nn.Sequential(\n",
    "            nn.Linear(in_features=256 , out_features=1024),  # Adjusted in_features based on flattened size\n",
    "            nn.Dropout(0.8),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.Linear(in_features=1024, out_features=NumberPitches)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for instrument detection\n",
    "        self.instrument = nn.Sequential(\n",
    "            nn.Linear(in_features=256 , out_features=512),  # Adjusted in_features based on flattened size\n",
    "            nn.Dropout(0.8),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.Linear(in_features=512, out_features= NumberInstruments)\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.BatchNorm2d):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "       # print('x_shape:',x.shape)\n",
    "       \n",
    "        x = self.sharedConvBlocks(x)  # Shared processing\n",
    "        \n",
    "\n",
    "        # Task-specific processing\n",
    "        pitchFeatures = self.pitchConvBlocks(x)\n",
    "        instrumentFeatures = self.instrumentConvBlocks(x)\n",
    "\n",
    "        # Apply adaptive pooling to make the output size consistent\n",
    "        pitchFeatures = self.adaptivePool(pitchFeatures)\n",
    "        instrumentFeatures = self.adaptivePool(instrumentFeatures)\n",
    "\n",
    "        # Flatten features for FC layers\n",
    "        pitchFeatures = torch.flatten(pitchFeatures, 1)\n",
    "        instrumentFeatures = torch.flatten(instrumentFeatures, 1)\n",
    "\n",
    "        # Final task-specific classification\n",
    "        pitchOut = self.pitch(pitchFeatures)\n",
    "        instrumentOut = self.instrument(instrumentFeatures)\n",
    "        #print('x_shape:',x.shape)\n",
    "       # print('instrumentOut:',instrumentOut.shape)\n",
    "        #print('pitchOut:',pitchOut.shape)\n",
    "\n",
    "        return pitchOut, instrumentOut\n",
    "model = MultiTaskCnnModel(config[\"CNN\"][\"NumberPitches\"], config[\"CNN\"][\"NumberInstruments\"], config[\"CNN\"][\"NumberOfBins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "def visualiseLossesTotal(trainLosses, validLosses, validAccuracies):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(trainLosses, label='Training')\n",
    "    ax1.plot(validLosses, label='Validation')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.set_ylim([0, 1.5]) \n",
    "    ax2.plot(validAccuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualiseLossesPitch(trainLosses, validLosses, validAccuracies):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    \n",
    "    ax1.plot(trainLosses, label='Training', color='purple')\n",
    "    ax1.plot(validLosses, label='Validation', color='silver')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.set_ylim([0, 1.5]) \n",
    "\n",
    "    ax2.plot( validAccuracies, label='Accuracy', color='purple')\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualiseLossesInstrument(trainLosses, validLosses, validAccuracies):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    \n",
    "    ax1.plot( trainLosses, label='Training', color='red')\n",
    "    ax1.plot( validLosses, label='Validation', color='pink')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.set_ylim([0, 1.5]) \n",
    "\n",
    "    ax2.plot(validAccuracies, label='Accuracy', color='red')\n",
    "    ax2.legend() # This was missing in your code to actually display the label\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def evaluate(model, dataLoader, pitchLoss, instrumentLoss,device):\n",
    "    model.eval()\n",
    "    numBatches = len(dataLoader)\n",
    "    pitchEpochLoss = instrumentEpochLoss = pitchAccuracy = instrumentAccuracy = 0\n",
    "    with torch.inference_mode():\n",
    "        for batchInputs, batchLabelsInstrument, batchLabelsPitch in dataLoader:\n",
    "            batchInputs = batchInputs.to(device)\n",
    "            batchLabelsPitch = batchLabelsPitch.to(device)\n",
    "            batchLabelsInstrument = batchLabelsInstrument.to(device)\n",
    "            pitchOutputs,instrumentOutputs = model(batchInputs)\n",
    "            pitchOutputs,instrumentOutputs = pitchOutputs.squeeze(),instrumentOutputs.squeeze()\n",
    "\n",
    "            pitchProbabilities = torch.sigmoid(pitchOutputs)\n",
    "            instrumentProbabilities = torch.sigmoid(instrumentOutputs)\n",
    "\n",
    "            # Convert probabilities to binary outputs\n",
    "            pitchBinaryOutputs = (pitchProbabilities >= 0.5).int()\n",
    "            instrumentBinaryOutputs = (instrumentProbabilities >= 0.5).int()\n",
    "\n",
    "            pitchAccuracy += (pitchBinaryOutputs == batchLabelsPitch).sum().item()\n",
    "            instrumentAccuracy += (instrumentBinaryOutputs == batchLabelsInstrument).sum().item()\n",
    "          \n",
    "            pitchEpochLoss += pitchLoss(pitchOutputs, batchLabelsPitch).item()\n",
    "            instrumentEpochLoss += instrumentLoss(instrumentOutputs, batchLabelsInstrument).item()\n",
    "           \n",
    "        \n",
    "        pitchEpochLoss /= numBatches\n",
    "        pitchAccuracy /= len(dataLoader.dataset)\n",
    "        \n",
    "        instrumentEpochLoss /= numBatches\n",
    "        instrumentAccuracy /= len(dataLoader.dataset)\n",
    "\n",
    "        totalEpochLoss = (pitchEpochLoss + instrumentEpochLoss)/2\n",
    "        totalAccuracy = (pitchAccuracy + instrumentAccuracy)/2\n",
    "    return totalEpochLoss, totalAccuracy, pitchEpochLoss, pitchAccuracy, instrumentEpochLoss, instrumentAccuracy \n",
    "\n",
    "   \n",
    "def train(model, trainLoader, validLoader, numEpochs, saved_model, optimizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    pitchLoss = nn.BCEWithLogitsLoss()\n",
    "    instrumentLoss = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler()\n",
    "    bestValidAcc = 0\n",
    "    earlyStopping = 100\n",
    "    evaluateEveryNEpochs = 10\n",
    "    \n",
    "    trainLosses, validLosses, validAccuracies, = [], [], []\n",
    "    pitchTrainLosses, pitchValidLosses, pitchValidAccuracies = [], [], []\n",
    "    instrumentTrainLosses, instrumentValidLosses, instrumentValidAccuracies = [], [], []\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        model.train()\n",
    "        totalTrainLoss= pitchTrainLoss= instrumentTrainLoss  = 0.0\n",
    "        for inputs, instrumentTargets, pitchTargets in trainLoader:\n",
    "            inputs, instrumentTargets, pitchTargets = inputs.to(device), instrumentTargets.to(device), pitchTargets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                pitchOutputs, instrumentOutputs = model(inputs)\n",
    "                lossPitch = pitchLoss(pitchOutputs, pitchTargets.float())\n",
    "                lossInstrument = instrumentLoss(instrumentOutputs, instrumentTargets.float())\n",
    "                totalLoss = lossPitch + lossInstrument\n",
    "            scaler.scale(totalLoss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            totalTrainLoss += totalLoss.item()\n",
    "            pitchTrainLoss += lossPitch.item()\n",
    "            instrumentTrainLoss += lossInstrument.item()\n",
    "        avgTotalTrainLoss = totalTrainLoss / len(trainLoader)\n",
    "        avgPitchTrainLoss = pitchTrainLoss / len(trainLoader)\n",
    "        avgInstrumentTrainLoss = instrumentTrainLoss / len(trainLoader)\n",
    "        trainLosses.append(avgTotalTrainLoss)\n",
    "        pitchTrainLosses.append(avgPitchTrainLoss)\n",
    "        instrumentTrainLosses.append(avgInstrumentTrainLoss)\n",
    "\n",
    "        totalValidLoss = totalValidAcc = pitchValidLoss = pitchValidAcc =InstrumentValidLoss= InstrumentValidAcc = 0\n",
    "        if((epoch+1)):\n",
    "            totalValidLoss, totalValidAcc, pitchValidLoss, pitchValidAcc, InstrumentValidLoss, InstrumentValidAcc = evaluate(model, validLoader, pitchLoss, instrumentLoss, device)\n",
    "            \n",
    "            validLosses.append(totalValidLoss)\n",
    "            validAccuracies.append(totalValidAcc)\n",
    "            pitchValidLosses.append(pitchValidLoss)\n",
    "            pitchValidAccuracies.append(pitchValidAcc)\n",
    "            instrumentValidLosses.append(InstrumentValidLoss)\n",
    "            instrumentValidAccuracies.append(InstrumentValidAcc)\n",
    "            print(f'Epoch {epoch + 1}/{numEpochs}, Total Train Loss: {trainLosses[-1]:.6f}, Total Validation Loss: {validLosses[-1]:.6f}, Total Validation Accuracy: {validAccuracies[-1]:.4f}')\n",
    "            print(f'Epoch {epoch + 1}/{numEpochs}, Pitch Train Loss: {pitchTrainLosses[-1]:.6f}, Pitch Validation Loss: {pitchValidLosses[-1]:.6f}, Pitch Validation Accuracy: {pitchValidAccuracies[-1]:.4f}')\n",
    "            print(f'Epoch {epoch + 1}/{numEpochs}, Instrument Train Loss: {instrumentTrainLosses[-1]:.6f}, Instrument Validation Loss: {instrumentValidLosses[-1]:.6f}, Instrument Validation Accuracy: {instrumentValidAccuracies[-1]:.4f}')\n",
    "        if((epoch+1) % evaluateEveryNEpochs == 0):\n",
    "            totalValidLoss, totalValidAcc, pitchValidLoss, pitchValidAcc, InstrumentValidLoss, InstrumentValidAcc = evaluate(model, validLoader, pitchLoss, instrumentLoss, device)\n",
    "            \n",
    "            validLosses.append(totalValidLoss)\n",
    "            validAccuracies.append(totalValidAcc)\n",
    "            pitchValidLosses.append(pitchValidLoss)\n",
    "            pitchValidAccuracies.append(pitchValidAcc)\n",
    "            instrumentValidLosses.append(InstrumentValidLoss)\n",
    "            instrumentValidAccuracies.append(InstrumentValidAcc)\n",
    "            visualiseLossesTotal(trainLosses, validLosses, validAccuracies)\n",
    "            visualiseLossesPitch(pitchTrainLosses, pitchValidLosses, pitchValidAccuracies)\n",
    "            visualiseLossesInstrument(instrumentTrainLosses, instrumentValidLosses, instrumentValidAccuracies)\n",
    "\n",
    "\n",
    "       \n",
    "        if(totalValidAcc >= bestValidAcc):\n",
    "            bestValidAcc = totalValidAcc\n",
    "            torch.save(model.state_dict(), saved_model)\n",
    "            print('Saving model with lowest validation loss')\n",
    "            epochsNoGood = 0\n",
    "        else:\n",
    "            epochsNoGood += 1\n",
    "            if epochsNoGood >= earlyStopping:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr= config['CNN']['LearningRate'],weight_decay= config['CNN']['Regularisation'])\n",
    "\n",
    "train(model, trainDataloader, validDataloader, config['CNN'][\"NumberEpochs\"], config[\"Path\"][\"SavedModels\"],optimizer )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
